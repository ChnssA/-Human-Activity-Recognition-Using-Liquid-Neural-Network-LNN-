{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChnssA/-Human-Activity-Recognition-Using-Liquid-Neural-Network-LNN-/blob/main/lnn_for_human_activity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely. This is a great practice for making your code more organized, modular, and reusable.\n",
        "\n",
        "I will split the single script into 5 separate Python files:\n",
        "\n",
        "1.  **`config.py`**: Holds all the constants and configuration settings.\n",
        "2.  **`data_loader.py`**: Contains the functions to load and preprocess the data.\n",
        "3.  **`models.py`**: Defines the `BaselineLSTM` and `LtcHARModel` classes.\n",
        "4.  **`utils.py`**: Includes helper functions for training, evaluating, and counting parameters.\n",
        "5.  **`main.py`**: The main script that you will run. It imports from the other files to build and run the experiment.\n",
        "\n",
        "### ðŸ“‚ New Project Structure\n",
        "\n",
        "Your project folder `LNN_HAR_Project` should now look like this:\n",
        "\n",
        "```\n",
        "LNN_HAR_Project/\n",
        "â”œâ”€â”€ venv/\n",
        "â”œâ”€â”€ UCI HAR Dataset/\n",
        "â”‚   â”œâ”€â”€ ... (all the dataset files)\n",
        "â”œâ”€â”€ config.py         # <-- NEW\n",
        "â”œâ”€â”€ data_loader.py    # <-- NEW\n",
        "â”œâ”€â”€ models.py         # <-- NEW\n",
        "â”œâ”€â”€ utils.py          # <-- NEW\n",
        "â””â”€â”€ main.py           # <-- NEW (replaces train_lnn.py)\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "### ðŸ Python Code for Each File\n",
        "\n",
        "Here is the complete code to copy and paste into each new file.\n",
        "\n",
        "#### 1\\. `config.py`\n",
        "\n",
        "(This file holds all your settings.)"
      ],
      "metadata": {
        "id": "t0ngttdMMdMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "DATASET_PATH = 'UCI HAR Dataset'\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 0.001\n",
        "HIDDEN_UNITS = 48\n",
        "NUM_CLASSES = 6\n",
        "INPUT_SIZE = 9  # 9 features (3x body_acc, 3x body_gyro, 3x total_acc)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "m-dSQ5o8MdMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2\\. `data_loader.py`\n",
        "\n",
        "(This file handles all data loading and preprocessing.)"
      ],
      "metadata": {
        "id": "6GnlcKePMdMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Import settings from config.py\n",
        "import config\n",
        "\n",
        "def load_har_data(split='train'):\n",
        "    \"\"\"\n",
        "    Loads the raw 9-channel time-series data from the 'Inertial Signals'\n",
        "    folders.\n",
        "    \"\"\"\n",
        "    print(f\"Loading {split} data...\")\n",
        "    signals_path = os.path.join(config.DATASET_PATH, split, 'Inertial Signals')\n",
        "\n",
        "    signal_files = [\n",
        "        'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
        "        'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
        "        'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
        "    ]\n",
        "\n",
        "    signal_data = []\n",
        "    for file_name in signal_files:\n",
        "        full_path = os.path.join(signals_path, f'{file_name}_{split}.txt')\n",
        "        data = pd.read_csv(full_path, delim_whitespace=True, header=None).values\n",
        "        signal_data.append(data)\n",
        "\n",
        "    # Stack along a new dimension (features) -> (samples, timesteps, features)\n",
        "    X = np.stack(signal_data, axis=-1)\n",
        "\n",
        "    # Load labels\n",
        "    label_path = os.path.join(config.DATASET_PATH, split, f'y_{split}.txt')\n",
        "    y = pd.read_csv(label_path, header=None).values.flatten()\n",
        "\n",
        "    # Adjust labels to be 0-indexed (original is 1-6)\n",
        "    y = y - 1\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def create_dataloaders(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Applies normalization and creates PyTorch DataLoaders.\n",
        "    \"\"\"\n",
        "    # Reshape for normalization: (samples * timesteps, features)\n",
        "    X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
        "    X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "    # Apply Z-score normalization\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled_2d = scaler.fit_transform(X_train_2d)\n",
        "    X_test_scaled_2d = scaler.transform(X_test_2d)\n",
        "\n",
        "    # Reshape back to 3D: (samples, timesteps, features)\n",
        "    X_train_scaled_3d = X_train_scaled_2d.reshape(X_train.shape)\n",
        "    X_test_scaled_3d = X_test_scaled_2d.reshape(X_test.shape)\n",
        "\n",
        "    # Convert to PyTorch Tensors\n",
        "    X_train_tensor = torch.tensor(X_train_scaled_3d, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled_3d, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    # Create TensorDatasets\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'config'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3183353549.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Import settings from config.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_har_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Z_u6oQR6MdMk",
        "outputId": "cb61fc94-3e4c-4ee1-c74b-9da91b20638b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3\\. `models.py`\n",
        "\n",
        "(This file defines your neural network architectures.)"
      ],
      "metadata": {
        "id": "QSPp4DkXMdMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from ncps.torch import LTC\n",
        "\n",
        "# A. Baseline Model (LSTM)\n",
        "class BaselineLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(BaselineLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # We only need the output of the last time step\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        # Get the last hidden state\n",
        "        out = h_n[-1]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# B. LNN Model (LTC Network)\n",
        "class LtcHARModel(nn.Module):\n",
        "    def __init__(self, input_size, units, num_classes):\n",
        "        super(LtcHARModel, self).__init__()\n",
        "        # Implements the LTC cell as the recurrent layer\n",
        "        self.ltc_layer = LTC(input_size, units)\n",
        "        # A dense softmax output layer\n",
        "        self.fc = nn.Linear(units, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # We only need the last hidden state for classification\n",
        "        _, h_n = self.ltc_layer(x)\n",
        "        out = self.fc(h_n)\n",
        "        return out"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "AtuSvlpaMdMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4\\. `utils.py`\n",
        "\n",
        "(This file contains the reusable helper functions.)"
      ],
      "metadata": {
        "id": "ReFwsYRTMdMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import config\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Helper function to count trainable parameters.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, epoch):\n",
        "    \"\"\"Performs one training epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{config.EPOCHS}], Train Loss: {avg_loss:.4f}')\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"Evaluates the model on the test set.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro') # Macro F1-score\n",
        "\n",
        "    return accuracy, f1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "81SUgZf5MdMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5\\. `main.py`\n",
        "\n",
        "(This is the main script you will run. It imports from all the files above.)"
      ],
      "metadata": {
        "id": "78qmvHupMdMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Import our custom modules\n",
        "import config\n",
        "from data_loader import load_har_data, create_dataloaders\n",
        "from models import BaselineLSTM, LtcHARModel\n",
        "from utils import train_model, evaluate_model, count_parameters\n",
        "\n",
        "def run_experiment():\n",
        "    # Load data\n",
        "    X_train, y_train = load_har_data('train')\n",
        "    X_test, y_test = load_har_data('test')\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader, test_loader = create_dataloaders(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    print(\"\\n--- Model Comparison ---\")\n",
        "\n",
        "    # --- Baseline LSTM ---\n",
        "    print(\"\\nTraining Baseline LSTM Model...\")\n",
        "    lstm_model = BaselineLSTM(\n",
        "        input_size=config.INPUT_SIZE,\n",
        "        hidden_size=config.HIDDEN_UNITS,\n",
        "        num_layers=2,\n",
        "        num_classes=config.NUM_CLASSES\n",
        "    )\n",
        "    criterion_lstm = nn.CrossEntropyLoss()\n",
        "    optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train_model(lstm_model, train_loader, criterion_lstm, optimizer_lstm, epoch)\n",
        "\n",
        "    lstm_accuracy, lstm_f1 = evaluate_model(lstm_model, test_loader)\n",
        "    lstm_params = count_parameters(lstm_model)\n",
        "\n",
        "    # --- LNN (LTC) Model ---\n",
        "    print(\"\\nTraining LNN (LTC) Model...\")\n",
        "    ltc_model = LtcHARModel(\n",
        "        input_size=config.INPUT_SIZE,\n",
        "        units=config.HIDDEN_UNITS,\n",
        "        num_classes=config.NUM_CLASSES\n",
        "    )\n",
        "    criterion_ltc = nn.CrossEntropyLoss()\n",
        "    optimizer_ltc = optim.Adam(ltc_model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train_model(ltc_model, train_loader, criterion_ltc, optimizer_ltc, epoch)\n",
        "\n",
        "    ltc_accuracy, ltc_f1 = evaluate_model(ltc_model, test_loader)\n",
        "    ltc_params = count_parameters(ltc_model)\n",
        "\n",
        "    # --- Final Results ---\n",
        "    print(\"\\n\\n--- ðŸ Final Results ---\")\n",
        "\n",
        "    print(\"\\nBaseline (LSTM):\")\n",
        "    print(f\"  Test Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
        "    print(f\"  Macro F1-Score: {lstm_f1:.4f}\")\n",
        "    print(f\"  Parameter Count: {lstm_params}\")\n",
        "\n",
        "    print(\"\\nLNN (LTC):\")\n",
        "    print(f\"  Test Accuracy: {ltc_accuracy * 100:.2f}%\")\n",
        "    print(f\"  Macro F1-Score: {ltc_f1:.4f}\")\n",
        "    print(f\"  Parameter Count: {ltc_params}\")\n",
        "\n",
        "    print(\"\\n--- Experiment Objectives Check ---\")\n",
        "    print(f\"LTC Accuracy >= 90%? {'Yes' if ltc_accuracy >= 0.9 else 'No'}\")\n",
        "\n",
        "    param_reduction = (lstm_params - ltc_params) / lstm_params\n",
        "    print(f\"Parameter Reduction: {param_reduction * 100:.2f}%\")\n",
        "    print(f\"Reduction >= 50%? {'Yes' if param_reduction >= 0.5 else 'No'}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "5BrXbN5RMdMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### How to Run\n",
        "\n",
        "After creating all 5 files, your VS Code setup should be complete. To run the entire project, you just need to run the `main.py` script from your activated terminal:\n",
        "\n",
        "```bash\n",
        "python main.py\n",
        "```\n",
        "\n",
        "The output will be identical to the previous \"big script,\" but your code is now much cleaner and easier to manage."
      ],
      "metadata": {
        "id": "Gr65CFL4MdMw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}